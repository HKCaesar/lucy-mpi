\section{Parallel division of the problem}

\subsection{Parallel sections}
There were two key areas in which the process of deblurring the images could be devided up for parrallel execution. The first and most obvious was that each image was completely indepentednt of the others meaning that multiple images could be worked on in parrallel. The second place the work could be made parrallel was in the convolution of the images with the point spread function. This could be split up as the result of one pixel in an image being convoluted was indepented of the results of the other pixels. Unlike the images however the result of a convolution of a pixel did depend on the value of nabouring pixels. This meant that a problem occoured with how to split the image up.

\subsection{Parrallelization overhead}
%If an image was convoluted by an n by n point spread function, then to calculate the value of a pixel in the image n^2 points of the image would need to be known. This presented a problem for a distributed system as it meant that if each pixel was to be calculated in its own process n^2 pixels would have to be transfered to the process. This problem could be reduced by transferring blocks of pixels to each process for them to transfer however it would reduce the amount performed in parrallel and still require additional transfer times. Assuming the size of the image is large compared to the size of the psf if a m by m block of pixels were transfered to each process then the number of pixels transfered to pixels output is given by (n+m)^2/(m^2).

If the convolution is performed on a machine with mulitple cores and shared memory however then no extra data transfers need to take place as all the data is already present.It also reduces the amount of data transfered in giving each process one image as it does not have to be first loaded into memory on the host and then transfered to the slaves memory